import { Injectable, Logger } from '@nestjs/common';
import { OpenAI } from 'openai';
import { ConfigService } from '@nestjs/config';
import { CacheService } from './cache.service';
import { RetryService } from './retry.service';
import { CircuitBreakerService } from './circuit-breaker.service';
import { AWSSecretManagerService } from './aws-secret.service';

@Injectable()
export class OpenAIService {
  private openai: OpenAI;
  private readonly logger = new Logger(OpenAIService.name);

  constructor(
    private configService: ConfigService,
    private cacheService: CacheService,
    private retryService: RetryService,
    private circuitBreakerService: CircuitBreakerService,
    private awsSecretManagerService: AWSSecretManagerService,
  ) { }

  async onModuleInit(): Promise<void> {
    const apiKey = await this.awsSecretManagerService.getSecretWithFallback(
      'openai-api-key',
      'OPENAI_API_KEY'
    );

    this.openai = new OpenAI({
      apiKey,
    });
    this.logger.log('OpenAI initialized with AWS Secret Manager');
  }

  async getEmbedding(text: string): Promise<number[]> {
    const cacheKey = this.cacheService.generateAIKey(text, 'embedding');

    return this.cacheService.getOrSet(
      cacheKey,
      () => this.retryService.withRetry({
        execute: () => this.circuitBreakerService.execute(
          'openai-embedding',
          async () => {
            const response = await this.openai.embeddings.create({
              model: this.configService.get<string>('OPENAI_EMBEDDING_MODEL') || 'text-embedding-3-small',
              input: text,
            });
            return response.data[0].embedding;
          }
        ),
        maxRetries: 2,
      }),
      86400
    );
  }

  async generateComplianceSummary(input: string): Promise<string> {
    const model = this.configService.get<string>('OPENAI_MODEL') || 'gpt-3.5-turbo';
    const cacheKey = this.cacheService.generateAIKey(input, model);

    return this.cacheService.getOrSet(
      cacheKey,
      () => this.retryService.withRetry({
        execute: () => this.circuitBreakerService.execute(
          'openai-chat',
          async () => {
            const response = await this.openai.chat.completions.create({
              model,
              messages: [
                {
                  role: 'system',
                  content: `You are a security compliance assistant specialized in SOC2 and ISO 27001.`,
                },
                {
                  role: 'user',
                  content: input,
                },
              ],
              temperature: 0.3,
              max_tokens: 2000,
            });

            const content = response.choices[0].message.content;
            if (!content) {
              throw new Error('No content generated by OpenAI');
            }

            return content;
          }
        ),
        maxRetries: 3,
        retryDelay: (retryCount) => 2000 * Math.pow(2, retryCount),
      }),
      7200
    );
  }

  async generateCustomSummary(
    input: string,
    systemPrompt: string,
    options: {
      temperature?: number;
      maxTokens?: number;
      model?: string;
      attempts?: number;
    } = {}
  ): Promise<string> {
    const model = options.model || this.configService.get<string>('OPENAI_MODEL') || 'gpt-3.5-turbo';
    const cacheKey = this.cacheService.generateAIKey(`${systemPrompt}:${input}${options.attempts}`, model);

    return this.cacheService.getOrSet(
      cacheKey,
      () => this.retryService.withRetry({
        execute: () => this.circuitBreakerService.execute(
          'openai-custom',
          async () => {
            const response = await this.openai.chat.completions.create({
              model,
              messages: [
                {
                  role: 'system',
                  content: systemPrompt,
                },
                {
                  role: 'user',
                  content: input,
                },
              ],
              temperature: options.temperature ?? 0.3,
              max_tokens: options.maxTokens ?? 2000,
            });

            const content = response.choices[0].message.content;
            if (!content) {
              throw new Error('No content generated by OpenAI');
            }

            return content;
          }
        ),
        maxRetries: 3,
        retryDelay: (retryCount) => 2000 * Math.pow(2, retryCount),
      }),
      3600
    );
  }

  // Method to check AI service health
  async healthCheck(): Promise<boolean> {
    try {
      await this.circuitBreakerService.execute(
        'openai-health',
        async () => {
          // Simple test request
          await this.openai.models.list();
        },
        { failureThreshold: 2, recoveryTimeout: 30000 }
      );
      return true;
    } catch (error) {
      return false;
    }
  }

  /**
   * Function calling for AI agents
  */
  async callWithFunctions(
    messages: any[],
    functions: any[],
    options: {
      temperature?: number;
      maxTokens?: number;
      model?: string;
    } = {}
  ): Promise<{
    message: any;
    functionCall?: {
      name: string;
      arguments: any;
    };
  }> {
    const model = options.model || this.configService.get<string>('OPENAI_MODEL') || 'gpt-4';

    return this.retryService.withRetry({
      execute: () => this.circuitBreakerService.execute(
        'openai-function-call',
        async () => {
          const response = await this.openai.chat.completions.create({
            model,
            messages,
            tools: functions.map(func => ({
              type: 'function',
              function: func
            })),
            tool_choice: 'auto',
            temperature: options.temperature ?? 0.1,
            max_tokens: options.maxTokens ?? 1000,
          });

          const choice = response.choices[0];
          const message = choice.message;

          // Check if the AI wants to call a function
          if (message.tool_calls && message.tool_calls.length > 0) {
            const toolCall = message.tool_calls[0];
            if (toolCall.type === 'function') {
              return {
                message,
                functionCall: {
                  name: toolCall.function.name,
                  arguments: JSON.parse(toolCall.function.arguments),
                },
              };
            }
          }

          return { message };
        }
      ),
      maxRetries: 2,
      retryDelay: (retryCount) => 1000 * Math.pow(2, retryCount),
    });
  }
}
