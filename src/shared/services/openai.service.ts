import { Injectable, Logger } from '@nestjs/common';
import { OpenAI } from 'openai';
import { ConfigService } from '@nestjs/config';
import { CacheService } from './cache.service';
import { RetryService } from './retry.service';
import { AWSSecretManagerService } from './aws-secret.service';

@Injectable()
export class OpenAIService {
  private openai: OpenAI;
  private readonly logger = new Logger(OpenAIService.name);

  constructor(
    private configService: ConfigService,
    private cacheService: CacheService,
    private retryService: RetryService,
    private awsSecretManagerService: AWSSecretManagerService,
  ) { }

  async onModuleInit(): Promise<void> {
    const apiKey = await this.awsSecretManagerService.getSecretWithFallback(
      'openai-api-key',
      'OPENAI_API_KEY'
    );

    this.openai = new OpenAI({
      apiKey,
    });
    this.logger.log('OpenAI initialized with AWS Secret Manager');
  }

  async getEmbedding(text: string): Promise<number[]> {
    if (!this.openai) {
      throw new Error('OpenAI is not initialized');
    }

    const cacheKey = this.cacheService.generateAIKey(text, 'embedding');

    return this.cacheService.getOrSet(
      cacheKey,
      () => this.retryService.withRetry({
        execute: async () => {
          const response = await this.openai.embeddings.create({
            model: this.configService.get<string>('OPENAI_EMBEDDING_MODEL') || 'text-embedding-3-small',
            input: text,
          });
          return response.data[0].embedding;
        },
        serviceName: 'openai-embedding',
        maxRetries: 2,
      }),
      86400
    );
  }

  async generateComplianceSummary(input: string): Promise<string> {
    if (!this.openai) {
      throw new Error('OpenAI is not initialized');
    }

    const model = this.configService.get<string>('OPENAI_MODEL') || 'gpt-3.5-turbo';
    const cacheKey = this.cacheService.generateAIKey(input, model);

    return this.cacheService.getOrSet(
      cacheKey,
      () => this.retryService.withRetry({
        execute: async () => {
          const response = await this.openai.chat.completions.create({
            model,
            messages: [
              {
                role: 'system',
                content: `You are a security compliance assistant specialized in SOC2 and ISO 27001.`,
              },
              {
                role: 'user',
                content: input,
              },
            ],
            temperature: 0.3,
            max_tokens: 2000,
          });

          const content = response.choices[0].message.content;
          if (!content) {
            throw new Error('No content generated by OpenAI');
          }

          return content;
        },
        serviceName: 'openai-chat',
        maxRetries: 3,
        retryDelay: (retryCount) => 2000 * Math.pow(2, retryCount),
      }),
      7200
    );
  }

  async generateCustomSummary(
    input: string,
    systemPrompt: string,
    options: {
      temperature?: number;
      maxTokens?: number;
      model?: string;
      attempts?: number;
    } = {}
  ): Promise<string> {
    if (!this.openai) {
      throw new Error('OpenAI is not initialized');
    }

    const model = options.model || this.configService.get<string>('OPENAI_MODEL') || 'gpt-3.5-turbo';
    const cacheKey = this.cacheService.generateAIKey(`${systemPrompt}:${input}${options.attempts}`, model);

    return this.cacheService.getOrSet(
      cacheKey,
      () => this.retryService.withRetry({
        execute: async () => {
          const response = await this.openai.chat.completions.create({
            model,
            messages: [
              {
                role: 'system',
                content: systemPrompt,
              },
              {
                role: 'user',
                content: input,
              },
            ],
            temperature: options.temperature ?? 0.3,
            max_tokens: options.maxTokens ?? 2000,
          });

          const content = response.choices[0].message.content;
          if (!content) {
            throw new Error('No content generated by OpenAI');
          }

          return content;
        },
        serviceName: 'openai-custom',
        maxRetries: 3,
        retryDelay: (retryCount) => 2000 * Math.pow(2, retryCount),
      }),
      3600
    );
  }

  /**
   * Function calling for AI agents
  */
  async callWithFunctions(
    messages: any[],
    functions: any[],
    options: {
      temperature?: number;
      maxTokens?: number;
      model?: string;
    } = {}
  ): Promise<{
    message: any;
    functionCall?: {
      name: string;
      arguments: any;
    };
  }> {
    if (!this.openai) {
      throw new Error('OpenAI is not initialized');
    }

    const model = options.model || this.configService.get<string>('OPENAI_MODEL') || 'gpt-4';

    return this.retryService.withRetry({
      execute: async () => {
        const response = await this.openai.chat.completions.create({
          model,
          messages,
          tools: functions.map(func => ({
            type: 'function',
            function: func
          })),
          tool_choice: 'auto',
          temperature: options.temperature ?? 0.1,
          max_tokens: options.maxTokens ?? 1000,
        });

        const choice = response.choices[0];
        const message = choice.message;

        // Check if the AI wants to call a function
        if (message.tool_calls && message.tool_calls.length > 0) {
          const toolCall = message.tool_calls[0];
          if (toolCall.type === 'function') {
            return {
              message,
              functionCall: {
                name: toolCall.function.name,
                arguments: JSON.parse(toolCall.function.arguments),
              },
            };
          }
        }

        return { message };
      },
      serviceName: 'openai-function-call',
      maxRetries: 2,
      retryDelay: (retryCount) => 1000 * Math.pow(2, retryCount),
    });
  }

  // Method to check AI service health
  async healthCheck(): Promise<boolean> {
    try {
      await this.openai.models.list();
      return true;
    } catch (error) {
      return false;
    }
  }
}
